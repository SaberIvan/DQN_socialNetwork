{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ccacc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fengboyang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Library functions\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "from matplotlib import animation\n",
    "import networkx.algorithms.centrality as nx_centrality\n",
    "from collections import deque\n",
    "from matplotlib import animation\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4989bf5",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629f456",
   "metadata": {},
   "source": [
    "## Training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d8f7e",
   "metadata": {},
   "source": [
    "The global parameters of the generate_random_graph function:\n",
    "1. FAKE_DIFF_ITER: the fake nodes diffusion iteration\n",
    "2. FAKE_SEED_NUM: the initial fake seed set number\n",
    "3. NODE_NUM: the graph nodes number\n",
    "4. EDGE_NUM: the graph edges number \n",
    "5. M_INDEX: the generate graph method index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "128bf1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_graph(difusion_iteration, method_index,fake_seed_num, num_nodes , num_edges , probability = 0.5,m = 2, radius = None, k_nearest_neighbor = None, degree = None, seed = None):\n",
    "    random_graph = nx.Graph()\n",
    "    if method_index == 0:\n",
    "        random_graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "\n",
    "    elif method_index == 1:\n",
    "        random_graph = nx.erdos_renyi_graph(num_nodes, probability)\n",
    "\n",
    "    elif method_index == 2:\n",
    "         # m means the edges number from the create point to existing point\n",
    "        random_graph = nx.barabasi_albert_graph(num_nodes, m)\n",
    "\n",
    "    elif method_index == 3:\n",
    "        random_graph = nx.watts_strogatz_graph(num_nodes, k_nearest_neighbor, probability)\n",
    "#     elif method_index == 4:\n",
    "#         random_graph=nx.gnc_graph(num_nodes)\n",
    "\n",
    "#     elif method_index == 4:\n",
    "#         random_graph = nx.random_geometric_graph(num_nodes, radius)   \n",
    "        \n",
    "#     elif method_index == 5:\n",
    "#         random_graph = nx.random_regular_graph(degree, num_nodes)\n",
    "\n",
    "#     elif method_index == 7:\n",
    "#         #p1 = nodes in the tree connected,  p2 = nodes in the tree body connected\n",
    "#         random_graph = nx.random_lobster(num_nodes, probability, probability)\n",
    "\n",
    "#     elif method_index == 8:\n",
    "#         p = probability # probability connected with its nearest neighbor\n",
    "#         q = probability # probability connected with other nodes\n",
    "#         random_graph = nx.newman_watts_strogatz_graph(num_nodes, p, q)\n",
    "\n",
    "#     elif method_index == 9:\n",
    "#         random_graph = nx.create_powerlaw_cluster_graph(num_nodes, k_nearest_neighbor, probability, seed=seed)\n",
    "\n",
    "#     elif method_index == 10:\n",
    "#         # A random maximal planar graph is a graph in which every possible edge that can be added without creating a cycle is included, \n",
    "#         # resulting in a planar graph with the maximum number of edges. \n",
    "#         random_graph = nx.random_maximal_planar_graph(num_nodes)\n",
    "\n",
    "#     elif method_index == 11:\n",
    "#         random_graph = nx.random_threshold_graph(num_nodes, probability)\n",
    "\n",
    "#     elif method_index == 12:\n",
    "#         random_graph = nx.create_forest_fire_graph(num_nodes, probability, probability, seed=seed)\n",
    "\n",
    "#     elif method_index == 13:\n",
    "#         k = k_nearest_neighbor # This k is different with KNN, it generates a scale-free network where new nodes are added with k initial edges\n",
    "#         random_graph = nx.extended_barabasi_albert_graph(num_nodes, k, probability)\n",
    "\n",
    "\n",
    "#     elif method_index == 15:\n",
    "#         # lollipop graph\n",
    "#         m = num_nodes // 2  # Number of nodes in the complete graph\n",
    "#         n = num_nodes - m - 1  # Number of nodes in the cycle graph\n",
    "\n",
    "#         G_complete = nx.complete_graph(m)\n",
    "#         G_cycle = nx.cycle_graph(n)\n",
    "\n",
    "#         # Connect the complete graph and cycle graph\n",
    "#         G_complete.add_edge(m-1, m)\n",
    "#         random_graph = nx.compose(G_complete, G_cycle)\n",
    "    \n",
    "    \n",
    "    for (u,v) in random_graph.edges:\n",
    "        if FIXED == True:\n",
    "            random_graph.edges[u, v][\"weight\"] = PROBABILITY\n",
    "        else:\n",
    "            random_graph.edges[u, v][\"weight\"] =  random.uniform(0,1)\n",
    "    for node in random_graph.nodes():\n",
    "        random_graph.nodes[node]['state'] = 0 # 0初始状态；1:true；2:fake\n",
    "    node_number = random_graph.number_of_nodes()\n",
    "    start_node_index = [0] *fake_seed_num\n",
    "    index_range = range(0,  node_number)\n",
    "    fake_seed_set = random.sample(index_range, fake_seed_num)\n",
    "    fake_active_nodes = fake_seed_set.copy()\n",
    "    fake_nodes_set = fake_seed_set.copy()\n",
    "    for _ in range(difusion_iteration):\n",
    "        temp_fake = []\n",
    "        for v in fake_active_nodes:\n",
    "            for nbr in random_graph.neighbors(v): \n",
    "                if random_graph.nodes[nbr]['state'] == 0 : \n",
    "                    edge_data = random_graph.get_edge_data(v, nbr)\n",
    "                    if random.uniform(0, 1) < edge_data['weight']:\n",
    "                        fake_nodes_set.append(nbr)\n",
    "                        temp_fake.append(nbr)\n",
    "                        random_graph.nodes[nbr]['state'] = 2\n",
    "        fake_active_nodes = temp_fake.copy()\n",
    "        temp_fake.clear()    \n",
    "    for node in fake_nodes_set:\n",
    "        random_graph.nodes[node]['state'] = 2\n",
    "        \n",
    "    return random_graph,fake_active_nodes,fake_seed_set,fake_nodes_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9152b3",
   "metadata": {},
   "source": [
    "## Create and normalize test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7621db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853ee652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc \n",
    "import scipy as sp\n",
    "import scipy.sparse  # call as sp.sparse\n",
    "\n",
    "# filename_ca_HepTh = r\"D:\\course (Kou Hari)\\2023 semaster 1\\COMP5703 Capstone\\data\\ca-HepTh.mtx\"\n",
    "# filename_ca_GrQc = r\"D:\\course (Kou Hari)\\2023 semaster 1\\COMP5703 Capstone\\data\\ca-GrQc.mtx\"\n",
    "# filename_tech_p2p_gnutella = r\"D:\\course (Kou Hari)\\2023 semaster 1\\COMP5703 Capstone\\data\\tech-p2p-gnutella.mtx\"\n",
    "filename_ca_HepTh = 'ca-HepTh.mtx'\n",
    "filename_ca_GrQc = 'ca-GrQc.mtx'\n",
    "filename_tech_p2p_gnutella = 'tech-p2p-gnutella.mtx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f6383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(filename):\n",
    "    adata = sc.read(filename)\n",
    "    G = nx.from_scipy_sparse_array(adata.X, create_using=nx.MultiGraph)\n",
    "    print(\"node number:\",G.number_of_nodes())\n",
    "    print(\"edge number:\",G.number_of_edges())\n",
    "    return G, G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b91f972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node number: 9877\n",
      "edge number: 25998\n"
     ]
    }
   ],
   "source": [
    "G_ca_HepTh,N_ca_HepTh,E_ca_HepTh= read_graph(filename_ca_HepTh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32accfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node number: 5242\n",
      "edge number: 14496\n"
     ]
    }
   ],
   "source": [
    "G_ca_GrQc,N_ca_GrQc,E_ca_GrQc = read_graph(filename_ca_GrQc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c030b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node number: 62561\n",
      "edge number: 147878\n"
     ]
    }
   ],
   "source": [
    "G_tech_p2p_gnutella,N_tech_p2p_gnutella,E_tech_p2p_gnutella = read_graph(filename_tech_p2p_gnutella)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f910e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_graph(graph):\n",
    "    graph_node_number = graph.number_of_nodes()\n",
    "    graph_edge_number = graph.number_of_edges()\n",
    "    for node in graph.nodes():\n",
    "        graph.nodes[node]['state'] = 0\n",
    "    for (u,v) in graph.edges:\n",
    "        graph.edges[u,v]['weight'] = random.uniform(0,1)\n",
    "    return graph,graph_node_number,graph_edge_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c568a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_nodes_diffusion(graph,fake_seed_num,iteration):\n",
    "    node_number = graph.number_of_nodes()\n",
    "    start_node_index = [0] *fake_seed_num\n",
    "    index_range = range(0,  node_number)\n",
    "    fake_seed_set = random.sample(index_range, fake_seed_num)\n",
    "    fake_active_nodes = fake_seed_set.copy()\n",
    "    fake_nodes_set = fake_seed_set.copy()\n",
    "    for node in fake_seed_set:\n",
    "        graph.nodes[node]['state'] = 2\n",
    "    #fake news diffusion\n",
    "    for _ in range(iteration):\n",
    "        temp_fake = []\n",
    "        for v in fake_active_nodes:\n",
    "            for nbr in graph.neighbors(v): \n",
    "                if graph.nodes[nbr]['state'] == 0 : \n",
    "                    edge_data = graph.get_edge_data(v, nbr)\n",
    "                    if random.uniform(0, 1) < edge_data['weight']:\n",
    "                        fake_nodes_set.append(nbr)\n",
    "                        temp_fake.append(nbr)\n",
    "                        graph.nodes[nbr]['state'] = 2\n",
    "        fake_active_nodes = temp_fake.copy()  \n",
    "    for node in fake_nodes_set:\n",
    "        random_graph.nodes[node]['state'] = 2\n",
    "    return graph,fake_active_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10525014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_draw_graph(Graph,draw = False):\n",
    "    color_list = []\n",
    "    fake = 0\n",
    "    true = 0\n",
    "    normal = 0\n",
    "    for node in Graph.nodes():\n",
    "        if Graph.nodes[node]['state'] == 0:\n",
    "            normal += 1\n",
    "            color_list.append('blue')\n",
    "        elif Graph.nodes[node]['state'] == 1:\n",
    "            true += 1\n",
    "            color_list.append('green')\n",
    "        elif Graph.nodes[node]['state'] == 2:\n",
    "            fake += 1\n",
    "            color_list.append('red')\n",
    "    print(\"normal nodes number:\",normal)\n",
    "    print(\"true nodes number:\",true)\n",
    "    print(\"fake nodes number:\",fake)\n",
    "    print(\"the edge of graph:\",Graph.number_of_edges())\n",
    "    #nx.draw(Graph, node_color= color_list) \n",
    "    if draw:\n",
    "        nx.draw_circular(Graph, node_color= color_list)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18455cb",
   "metadata": {},
   "source": [
    "# Environment Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d374128b",
   "metadata": {},
   "source": [
    "The global parameters in the Environment Class:\n",
    "1. ITERATION: control the diffusion times during the true and fake nodes antagonistic process\n",
    "2. MAX_STEP: control the max step of this epoch\n",
    "3. SEED_SIZE: the initial size of the seed set, the initial parameters of Env class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538ecf0",
   "metadata": {},
   "source": [
    "The input variable of the Env() class:\n",
    "1. graph: the training graph or testing graph\n",
    "2. seed_size: SEED_SIZE;the initial size of the seed set\n",
    "3. fake_set: the initial fake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0906a",
   "metadata": {},
   "source": [
    "The functions in the Env class:\n",
    "1. reset: reset the parameters\n",
    "2. step: the experiment excuation progress\n",
    "3. select_initial_seeds: select the initial seed randomly\n",
    "4. select_initial_seeds_rules: select the intial seed set according to different criterion\n",
    "5. add_seed: add the seed nodes according to the action number from the agent\n",
    "6. get_state: get the current state of the graph\n",
    "7. diffusion_process: the procees of the true and fake nodes antagonistic\n",
    "8. get_image: draw the image of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4f87db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self, graph,seed_size,fake_set):\n",
    "        self.graph_initial = graph\n",
    "        self.seed_size = seed_size\n",
    "        self.seed_set = []\n",
    "        self.fake_set = fake_set\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        #select the seed set:\n",
    "        self.graph = self.graph_initial.copy()\n",
    "        self.step_count = 0\n",
    "        self.add_number = 0\n",
    "        self.seed_set, action = self.select_initial_seeds_rules(self.graph,self.seed_size)\n",
    "        self.true_active_nodes = self.seed_set.copy()\n",
    "        self.fake_active_nodes = self.fake_set.copy()\n",
    "        self.state = self.get_state()\n",
    "        return self.state, action\n",
    "    \n",
    "    def step(self,action):\n",
    "        count_state0 = 0\n",
    "        for node in self.graph.nodes():\n",
    "            if self.graph.nodes[node]['state'] == 0:\n",
    "                count_state0 += 1\n",
    "        if action > 0 and (len(self.seed_set) + ADD_SEED_NUMBER <= self.graph.number_of_nodes()) and count_state0 >= ADD_SEED_NUMBER:\n",
    "            self.add_number += 1\n",
    "            centrality_methods = [\"None\", \"random\", \"degree\", \"closeness\", \"betweenness\", \"eigenvector\"]\n",
    "            centrality_method = centrality_methods[action]\n",
    "            self.seed_set = self.add_seed(self.graph,self.seed_set,action,ADD_SEED_NUMBER)\n",
    "            if(self.true_active_nodes[-ADD_SEED_NUMBER:]!= self.seed_set[-ADD_SEED_NUMBER:]):\n",
    "                self.true_active_nodes.extend(self.seed_set[-ADD_SEED_NUMBER:])\n",
    "        self.graph,self.true_active_nodes,self.fake_active_nodes = self.diffusion_process(self.graph,\n",
    "                                                                                     self.true_active_nodes,\n",
    "                                                                               self.fake_active_nodes,ITERATION)\n",
    "        #print(\"current image:\")\n",
    "        #self.get_image(self.graph)\n",
    "        \n",
    "        # terminal condition                                                                    \n",
    "        all_true_nodes = []\n",
    "        all_fake_nodes = []\n",
    "        normal_nodes= []                                                                             \n",
    "        for node in self.graph.nodes():\n",
    "            if self.graph.nodes[node]['state'] == 1:\n",
    "                all_true_nodes.append(node)\n",
    "            elif self.graph.nodes[node]['state'] == 2:\n",
    "                all_fake_nodes.append(node)\n",
    "            elif self.graph.nodes[node]['state'] == 0:\n",
    "                normal_nodes.append(node)\n",
    "        number_nodes = len(self.graph.nodes())\n",
    "        number_true_nodes = len(all_true_nodes) \n",
    "        number_fake_nodes = len(all_fake_nodes)\n",
    "        # terminal condition: over the MAX_STEP / less 1% nodes are normal nodes\n",
    "        terminated = bool(self.step_count >= MAX_STEP \n",
    "                        or len(normal_nodes)<=0.01 * number_nodes) \n",
    "#         penatly = math.pow(1.2,self.add_number)\n",
    "#         print(\"penatly:\",penatly)\n",
    "#         print(len(all_true_nodes)  - len(all_fake_nodes))\n",
    "        penatly = math.pow(1.05,self.add_number)\n",
    "        if not terminated:\n",
    "            if len(all_true_nodes)>len(all_fake_nodes):\n",
    "                reward = (len(all_true_nodes)  - len(all_fake_nodes) - penatly + 5)/(len(all_true_nodes) + len(all_fake_nodes))\n",
    "                #print(\">:\",reward)\n",
    "            else:\n",
    "                reward = (len(all_true_nodes)  - len(all_fake_nodes) - penatly)/(len(all_true_nodes) + len(all_fake_nodes))\n",
    "                \n",
    "                #print(\"<:\",reward)\n",
    "            done = False\n",
    "            self.step_count +=1\n",
    "        else: \n",
    "            # when this iteration is end, give a little big reward.\n",
    "            reward = 30\n",
    "            done = True\n",
    "#         if not terminated:\n",
    "#             if len(all_true_nodes)>len(all_fake_nodes):\n",
    "#                 reward = (len(all_true_nodes)  - len(all_fake_nodes) - penatly )/(len(all_true_nodes) + len(all_fake_nodes))#+0.7\n",
    "#                 print(\">:\",reward)\n",
    "#             else:\n",
    "#                 reward = (len(all_true_nodes)  - len(all_fake_nodes))/(len(all_true_nodes) + len(all_fake_nodes)) #-0.5\n",
    "#                 print(\"<:\",reward)\n",
    "#             done = False\n",
    "#             self.step_count +=1\n",
    "#         else:\n",
    "#             reward = 10 +  (MAX_STEP - self.step_count) * 0.25\n",
    "#             done = True\n",
    "            \n",
    "#             self.step_count >= MAX_STEP: \n",
    "#             # when this iteration is end, give a little big reward.\n",
    "#             reward = 20\n",
    "#             done = True\n",
    "#         elif len(normal_nodes)<=0.01 * number_nodes:\n",
    "#             reward = 30\n",
    "#             done = True\n",
    "        self.state = self.get_state()\n",
    "        return self.state, reward ,done \n",
    "    # select the initial seed set randomly\n",
    "    def select_initial_seeds(self, graph, seed_number):\n",
    "        node_number = graph.number_of_nodes()\n",
    "        start_node_index = [0] *seed_number\n",
    "        index_range = range(0,  node_number)\n",
    "        start_node_index = random.sample(index_range, seed_number)\n",
    "        for node in graph.nodes():\n",
    "            if node in start_node_index:\n",
    "                graph.nodes[node]['state'] = 1\n",
    "        return start_node_index\n",
    "    # select the initial seed set according to different rules\n",
    "    def select_initial_seeds_rules(self,graph,seed_number):\n",
    "        # 1 random 2 \"degree\",3 \"closeness\", 4\"betweenness\", 5\"eigenvector\"\n",
    "        action = random.randint(0,5)\n",
    "        start_node_index = []\n",
    "        centrality_scores = []\n",
    "        if action == 0:   \n",
    "            node_number = graph.number_of_nodes()\n",
    "            start_node_index = [0] *seed_number\n",
    "            index_range = range(0,  node_number)\n",
    "            start_node_index = random.sample(index_range, seed_number)\n",
    "        else:\n",
    "            if action == 1:\n",
    "                centrality_scores  = list(nx_centrality.degree_centrality(self.graph).values())\n",
    "            elif action == 2:\n",
    "                centrality_scores  = list(nx_centrality.degree_centrality(self.graph).values())\n",
    "            elif action == 3:\n",
    "                centrality_scores = list(nx_centrality.closeness_centrality(self.graph).values())\n",
    "            elif action == 4:\n",
    "                centrality_scores = list(nx_centrality.betweenness_centrality(self.graph).values())\n",
    "            elif action == 5:\n",
    "                centrality_scores = list(nx_centrality.eigenvector_centrality(self.graph).values())\n",
    "            for _ in range(seed_number):\n",
    "                max_number = max(centrality_scores)\n",
    "                index = centrality_scores.index(max_number)\n",
    "                while(graph.nodes[index]['state'] != 0):\n",
    "                    centrality_scores[index] = -1\n",
    "                    max_number = max(centrality_scores)\n",
    "                    index = centrality_scores.index(max_number)  \n",
    "                start_node_index.append(index)\n",
    "                centrality_scores[index] = -1\n",
    "        for node in graph.nodes():\n",
    "            if node in start_node_index:\n",
    "                graph.nodes[node]['state'] = 1\n",
    "        return start_node_index,action\n",
    "        \n",
    "    def add_seed(self,graph,seed_set,action,add_number):\n",
    "        # 1 random 2 \"degree\",3 \"closeness\", 4\"betweenness\", 5\"eigenvector\"\n",
    "        new_seed_set = []\n",
    "        new_seed_set = seed_set.copy()\n",
    "        centrality_scores = []\n",
    "        state0 = 0\n",
    "        for node in graph.nodes():\n",
    "            if graph.nodes[node]['state'] == 0:\n",
    "                state0 += 1\n",
    "        if state0 >= ADD_SEED_NUMBER:\n",
    "            if action == 1:\n",
    "                node_number = graph.number_of_nodes()\n",
    "                for _ in range(ADD_SEED_NUMBER):\n",
    "                    index = random.randint(0,node_number-1)\n",
    "                    while (index in seed_set) or (graph.nodes[index]['state'] in [1,2])  :\n",
    "                        index = random.randint(0,node_number-1)\n",
    "                    new_seed_set.append(index)\n",
    "                    graph.nodes[index]['state']  = 1\n",
    "\n",
    "            else: \n",
    "                if action == 2:\n",
    "                    centrality_scores  = list(nx_centrality.degree_centrality(self.graph).values())\n",
    "                elif action == 3:\n",
    "                    centrality_scores = list(nx_centrality.closeness_centrality(self.graph).values())\n",
    "                elif action == 4:\n",
    "                    centrality_scores = list(nx_centrality.betweenness_centrality(self.graph).values())\n",
    "                elif action == 5:\n",
    "                    centrality_scores = list(nx_centrality.eigenvector_centrality(self.graph).values())\n",
    "                for node in seed_set:\n",
    "                    centrality_scores[node] = -1\n",
    "                for _ in range(ADD_SEED_NUMBER):\n",
    "                    index = centrality_scores.index(max(centrality_scores))\n",
    "                    while(graph.nodes[index]['state'] != 0):\n",
    "                        index = centrality_scores.index(max(centrality_scores))\n",
    "                        # print(index)\n",
    "                        centrality_scores[index] = -1\n",
    "                    # print(\"add_seed, index:\",index)\n",
    "                    new_seed_set.append(index)\n",
    "                    graph.nodes[index]['state'] = 1\n",
    "                    centrality_scores[index] = -1\n",
    "        return new_seed_set\n",
    "    def get_state(self):\n",
    "        node_state = []\n",
    "        for node in self.graph.nodes():\n",
    "            node_state.append(self.graph.nodes[node]['state'])\n",
    "        return nx.to_numpy_array(self.graph), len(self.seed_set), node_state\n",
    "    \n",
    "    def diffusion_process(self, G ,true_active_set,fake_active_set,itertaion):\n",
    "        G = self.graph.copy()\n",
    "        true_active_nodes = true_active_set.copy()\n",
    "        fake_active_nodes = fake_active_set.copy()\n",
    "        # print(\"true_active_nodes begin:\",true_active_nodes)\n",
    "        # print(\"fake_active_nodes begin:\",fake_active_nodes)\n",
    "        for _ in range(itertaion):\n",
    "            tmp_true_nodes ={}\n",
    "            for v in true_active_nodes: \n",
    "                for nbr in G.neighbors(v):\n",
    "                    if G.nodes[nbr]['state'] in [0]:\n",
    "                        edge_data = G.get_edge_data(v, nbr)\n",
    "                        random_possibility = random.uniform(0, 1)\n",
    "                        if random_possibility < edge_data['weight']:\n",
    "                            G.nodes[nbr]['state'] == 3\n",
    "                            tmp_true_nodes.update({nbr:random_possibility})\n",
    "                            true_active_set.append(nbr)\n",
    "                    else:\n",
    "                        continue\n",
    "            tmp_fake_nodes ={}\n",
    "            for v in fake_active_nodes:\n",
    "                for nbr in G.neighbors(v):\n",
    "                    if G.nodes[nbr]['state'] in [0,3]:\n",
    "                        edge_data = G.get_edge_data(v, nbr)\n",
    "                        random_possibility = random.uniform(0, 1)\n",
    "                        if random_possibility < edge_data['weight']:\n",
    "                            tmp_fake_nodes.update({nbr:random_possibility})\n",
    "                            fake_active_set.append(nbr)\n",
    "                            G.nodes[nbr]['state'] == 4\n",
    "            true_active_nodes = list(tmp_true_nodes.keys())\n",
    "            fake_active_nodes = list(tmp_fake_nodes.keys())\n",
    "            for node in tmp_true_nodes.keys():\n",
    "                if node in tmp_fake_nodes.keys() :\n",
    "                    if tmp_true_nodes.get(node) >= tmp_fake_nodes.get(node):\n",
    "                        fake_active_set.remove(node)\n",
    "                        fake_active_nodes.remove(node)\n",
    "                    elif tmp_true_nodes.get(node) < tmp_fake_nodes.get(node):\n",
    "                        true_active_set.remove(node)\n",
    "                        true_active_nodes.remove(node)\n",
    "                else:\n",
    "                    continue\n",
    "            for node in true_active_set:\n",
    "                G.nodes[node]['state'] = 1\n",
    "            for node in fake_active_set:\n",
    "                G.nodes[node]['state'] = 2\n",
    "            # print(\"true_active_nodes\",true_active_set)\n",
    "            # print(\"fake_active_nodes\",fake_active_set)\n",
    "        return G,true_active_nodes,fake_active_nodes\n",
    "\n",
    "        \n",
    "    def get_image(self,Graph):\n",
    "        color_list = []\n",
    "        for node in Graph.nodes():\n",
    "            if Graph.nodes[node]['state'] == 0:\n",
    "                color_list.append('blue')\n",
    "            elif Graph.nodes[node]['state'] == 1:\n",
    "                color_list.append('green')\n",
    "            elif Graph.nodes[node]['state'] == 2:\n",
    "                color_list.append('red')\n",
    "        #nx.draw(Graph, node_color= color_list) \n",
    "        nx.draw_circular(Graph, node_color= color_list)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b70ee",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927aca8",
   "metadata": {},
   "source": [
    "##  Device: GPU/CPU\n",
    "Note:\n",
    "\n",
    "if you use Macbook with Apple M1/M2 core, please run the code in the Mac Device;\n",
    "\n",
    "if you use Windows with CUDA core, please run the code in the Windows Device;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0eca57",
   "metadata": {},
   "source": [
    "### Mac Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e45e7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# # if use MAC(Apple M1/M2) please run the follow codes\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c19cb34",
   "metadata": {},
   "source": [
    "### Windows Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ee6999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f60eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name()\n",
    "    print(\"GPU Device Name:\", device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7849d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.cudnn.is_available():\n",
    "    print(\"cuDNN Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79d0e1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# if use CUDA please run the follow codes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ca8d9",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2821ae4",
   "metadata": {},
   "source": [
    "The input variable of the DQNAgent() class link to the global parameters:\n",
    "1. state_dim: STATE_DIM; Dimension of state\n",
    "2. action_dim: ACT_DIM; Dimension of action space \n",
    "3. learning_rate: LR;\n",
    "4. gamma: GAMMA\n",
    "5. epsilon: EPSILON\n",
    "6. epsilon_decay: EPSILON_DECAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7015c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate, gamma, epsilon, epsilon_decay):\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, action_dim)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            q_values = self.q_network(state)\n",
    "#           q_values = self.q_network(torch.tensor(state, dtype=torch.float32))\n",
    "            return torch.argmax(q_values.cpu()).item()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #state = torch.tensor(state).float().unsqueeze(0)  # ensure state is a tensor\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #print(\"next_state:\", next_state)\n",
    "            #state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            state_tensor = state_to_tensor(state).to(device)\n",
    "            #print(state_tensor.shape)\n",
    "            #print(self.q_network(state_tensor).shape)\n",
    "            action_index = torch.tensor(action, dtype=torch.long)\n",
    "            target = self.q_network(state_tensor)[0, action_index]\n",
    "            #target = self.q_network(state_tensor)[action]\n",
    "            if done:\n",
    "                target_value = reward\n",
    "            else:\n",
    "                next_state_tensor = state_to_tensor(next_state).to(device)\n",
    "                #next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "                next_q_values = self.target_network(next_state_tensor).detach()\n",
    "                target_value = reward + self.gamma * torch.max(next_q_values).item()\n",
    "\n",
    "            #loss = self.loss_fn(target, torch.tensor(float(target_value)))\n",
    "            loss = self.loss_fn(target, torch.tensor(float(target_value)).to(device))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.q_network.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.q_network.load_state_dict(torch.load(path))\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea207cf5",
   "metadata": {},
   "source": [
    "### Transform array state to tensor state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4c023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tensor(state):\n",
    "    graph, num_seed_nodes, node_state = state\n",
    "    \n",
    "    graph_tensor = torch.tensor(graph, dtype=torch.float32).view(-1).unsqueeze(0)\n",
    "    num_seed_nodes_tensor = torch.tensor([num_seed_nodes], dtype=torch.float32).unsqueeze(0)\n",
    "    node_state_tensor = torch.tensor(node_state, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "#     state_tensor = torch.cat((graph_tensor, num_seed_nodes_tensor, node_state_tensor), dim=1)\n",
    "    state_tensor = torch.cat((num_seed_nodes_tensor, node_state_tensor), dim=1)\n",
    "    \n",
    "    return state_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad22c0",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a84eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)# (2500 + 50 + 2) (10000 + 100 + 2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c497fb57",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "The input valiable of train_dqn function:\n",
    "\n",
    "1. agent: the initialized Agent class\n",
    "2. env: the initialized Env class\n",
    "3. batch_size: BATCH_SIZE\n",
    "4. update_target_every: UPDATE;How many iterations to update the network\n",
    "5. dqn_agent_name: the name of DQN agent, the file extension is '.pth'\n",
    "6. brenchmark_action: Start benchmark comparison and specifying an action(1-5)\n",
    "7. Demo: Whether state should be displayed or saved each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325ba526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, env, episodes, batch_size, update_target_every,dqn_agent_name  = None,brenchmark_action = None,Demo = False):\n",
    "    state_history= []\n",
    "    rewards_list = []\n",
    "    for episode in range(episodes):\n",
    "        state,init_action = env.reset()\n",
    "        if brenchmark_action is not None:\n",
    "            init_action = brenchmark_action\n",
    "        #print(state)\n",
    "#         state = torch.tensor(state).float().unsqueeze(0)  # ensure state is a tensor\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = state_to_tensor(state)\n",
    "            #print(state_tensor)\n",
    "            if brenchmark_action is not None:\n",
    "                action = brenchmark_action\n",
    "            else:\n",
    "                action = agent.get_action(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            #print(next_state)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.train(batch_size)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if Demo:\n",
    "                state_history.append(state)  # Add current state to state_history\n",
    "        if not Demo:\n",
    "            state_history.append(state)\n",
    "        rewards_list.append(total_reward)\n",
    "        if episode % update_target_every == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "    if dqn_agent_name is not None:\n",
    "        agent.save(dqn_agent_name)\n",
    "    return state_history, rewards_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917ab9b",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27ad5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(agent, env, episodes):\n",
    "    for episode in range(episodes):\n",
    "        state,init_action = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_tensor = state_to_tensor(state)\n",
    "            action = agent.get_action(state_tensor)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Test Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760da568",
   "metadata": {},
   "source": [
    "# Display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d3e63",
   "metadata": {},
   "source": [
    "## Display the training or testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71dfcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bde6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state2graph(state):\n",
    "    graph = nx.from_numpy_array(state[0])\n",
    "    #print(graph.nodes())\n",
    "    for node in graph.nodes():\n",
    "        graph.nodes[node]['state'] = state[2][node]\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2773dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_color_list(graph):\n",
    "    color_list = []\n",
    "    #create color list\n",
    "    #print(graph)\n",
    "    for node in graph.nodes():\n",
    "        if graph.nodes[node]['state'] == 0:\n",
    "            color_list.append('blue')\n",
    "        elif graph.nodes[node]['state'] == 1:\n",
    "            color_list.append('green')\n",
    "        elif graph.nodes[node]['state'] == 2:\n",
    "            color_list.append('red')\n",
    "    return color_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "567079d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_pos(state):\n",
    "    graph = nx.from_numpy_array(state[0])\n",
    "    position = spring_layout(graph)\n",
    "    position = nx.circular_layout(graph)   \n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a05f3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(state):\n",
    "    fig, ax = plt.subplots()\n",
    "    graph = state2graph(state[0])\n",
    "    pos = define_pos(state[0])\n",
    "    color_list = create_color_list(graph)\n",
    "    nx.draw(graph, pos, node_color= color_list)\n",
    "    \n",
    "    def animate(frame):\n",
    "        ax.clear()\n",
    "        #print(\"frame,\",frame)\n",
    "        #print(state[frame])\n",
    "        graph = state2graph(state[frame])\n",
    "        color_list = create_color_list(graph)\n",
    "        nx.draw(graph, pos, node_color= color_list)\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(state), interval=200)\n",
    "    anim.save('train.gif', writer='PillowWriter')\n",
    "    anim.save(\"train.mp4\",writer='ffmpeg')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa43bda",
   "metadata": {},
   "source": [
    "## Show the change of reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c31540bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_image(rewards_list):\n",
    "    x = range(1,ALL_EPISODES+1)\n",
    "    print(x)\n",
    "    plt.plot(x,rewards_list,'s-')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Total reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7d331",
   "metadata": {},
   "source": [
    "# Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "646becd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Global parameter of generate_random_graph\n",
    "FAKE_DIFF_ITER = 1 # the fake nodes diffusion iteration\n",
    "FAKE_SEED_NUM = 3 # the initial fake seed set number\n",
    "NODE_NUM = 100 # the intial graph nodes number\n",
    "EDGE_NUM = 200 # the intial graph edges number\n",
    "M_INDEX = 0 # the generate graph method index\n",
    "\n",
    "# the Global parameter of Env class\n",
    "ITERATION = 1 #control the diffusion times during the true and fake nodes antagonistic process\n",
    "MAX_STEP = 40 # control the max step of this epoch\n",
    "SEED_SIZE = 10 # the initial size of the seed set, the initial parameters of Env class.\n",
    "ADD_SEED_NUMBER = 1 # the number of nodes that are added to seed set\n",
    "\n",
    "# the Global parameter of Agentclass\n",
    "state_dim = NODE_NUM + 1 #state_dim: Dimension of state\n",
    "action_dim = 6 #action_dim:  Dimension of action space\n",
    "LR = 0.001#learning_rate: \n",
    "GAMMA = 0.99 # gamma:\n",
    "EPSILON = 1.0# epsilon: \n",
    "EPSILON_DECAY = 0.995 # epsilon_decay: \n",
    "# the Global parameter of train and test function\n",
    "batch_size = 128 #batch_size: \n",
    "UPDATE = 10 #update_target_every: \n",
    "ALL_EPISODES = 200 # total number of iteration times\n",
    "\n",
    "FIXED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b593d",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523e2e1",
   "metadata": {},
   "source": [
    "# 4.Change the diffusion iteration of initial fake set\n",
    "The iteration could be 0,1,2,3; Change\n",
    "比较模型在劣势环境下的状况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a1339ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Global parameter of generate_random_graph\n",
    "FAKE_DIFF_ITER = 1 # the fake nodes diffusion iteration\n",
    "FAKE_SEED_NUM = 5 # the initial fake seed set number\n",
    "NODE_NUM = 500 # the intial graph nodes number\n",
    "EDGE_NUM = 1500 # the intial graph edges number\n",
    "M_INDEX = 0 # the generate graph method index\n",
    "\n",
    "# the Global parameter of Env class\n",
    "ITERATION = 1 #control the diffusion times during the true and fake nodes antagonistic process\n",
    "MAX_STEP = 40 # control the max step of this epoch\n",
    "SEED_SIZE = 10 # the initial size of the seed set, the initial parameters of Env class.\n",
    "ADD_SEED_NUMBER = 1 # the number of nodes that are added to seed set\n",
    "\n",
    "# the Global parameter of Agentclass\n",
    "state_dim = NODE_NUM + 1 #state_dim: Dimension of state\n",
    "action_dim = 6 #action_dim:  Dimension of action space\n",
    "LR = 0.001#learning_rate: \n",
    "GAMMA = 0.99 # gamma:\n",
    "EPSILON = 1.0# epsilon: \n",
    "EPSILON_DECAY = 0.995 # epsilon_decay: \n",
    "# the Global parameter of train and test function\n",
    "batch_size = 128 #batch_size: \n",
    "UPDATE = 10 #update_target_every: \n",
    "ALL_EPISODES = 200 # total number of iteration times\n",
    "\n",
    "FIXED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dff277",
   "metadata": {},
   "source": [
    "### FAKE_DIFF_ITER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "51829111",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE_DIFF_ITER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2c86475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_train_graph_fake_0,active_fake_set_fake_0,intial_fake_set_fake_0,all_fake_nodes_fake_0 = generate_random_graph(FAKE_DIFF_ITER,M_INDEX,FAKE_SEED_NUM,NODE_NUM, EDGE_NUM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2f430bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(active_fake_set_fake_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c0d98571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "SEED_SIZE = len(active_fake_set_fake_0)\n",
    "print(SEED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3c4092cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fake_0 = Env(intial_train_graph_fake_0, SEED_SIZE,active_fake_set_fake_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "16656022",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fake_0 = DQNAgent(state_dim, action_dim, LR , GAMMA, EPSILON, EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "7db2dc70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Total Reward: 55.462503273186\n",
      "Episode 2/200, Total Reward: 51.554632710271036\n",
      "Episode 3/200, Total Reward: 66.06978214735864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/bz9f8bp52sv9260n6zps4vbr0000gn/T/ipykernel_1713/545150970.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4/200, Total Reward: 58.8514719400912\n",
      "Episode 5/200, Total Reward: 64.09941074277214\n",
      "Episode 6/200, Total Reward: 65.57239386106093\n",
      "Episode 7/200, Total Reward: 64.32077749411373\n",
      "Episode 8/200, Total Reward: 63.423120924871796\n",
      "Episode 9/200, Total Reward: 63.34630197112166\n",
      "Episode 10/200, Total Reward: 61.49311562635795\n",
      "Episode 11/200, Total Reward: 65.20484497795422\n",
      "Episode 12/200, Total Reward: 64.76253223159634\n",
      "Episode 13/200, Total Reward: 56.044333215136845\n",
      "Episode 14/200, Total Reward: 59.28168719640949\n",
      "Episode 15/200, Total Reward: 64.78628345435953\n",
      "Episode 16/200, Total Reward: 41.82419028662589\n",
      "Episode 17/200, Total Reward: 65.37648449046964\n",
      "Episode 18/200, Total Reward: 66.09691285859321\n",
      "Episode 19/200, Total Reward: 58.20795414368021\n",
      "Episode 20/200, Total Reward: 60.71067519349766\n",
      "Episode 21/200, Total Reward: 27.91260277917694\n",
      "Episode 22/200, Total Reward: 61.100996764189134\n",
      "Episode 23/200, Total Reward: 60.13361869502393\n",
      "Episode 24/200, Total Reward: 64.38940386040572\n",
      "Episode 25/200, Total Reward: 53.78215019849607\n",
      "Episode 26/200, Total Reward: 34.523879857928215\n",
      "Episode 27/200, Total Reward: 65.673534147293\n",
      "Episode 28/200, Total Reward: 63.06719921179883\n",
      "Episode 29/200, Total Reward: 65.55931506837241\n",
      "Episode 30/200, Total Reward: 62.266089428384525\n",
      "Episode 31/200, Total Reward: 61.44852195604658\n",
      "Episode 32/200, Total Reward: 65.17267121387783\n",
      "Episode 33/200, Total Reward: 61.69438295988354\n",
      "Episode 34/200, Total Reward: 54.47946441758353\n",
      "Episode 35/200, Total Reward: 61.189060620174374\n",
      "Episode 36/200, Total Reward: 63.1461976032973\n",
      "Episode 37/200, Total Reward: 41.65861810170475\n",
      "Episode 38/200, Total Reward: 66.56941897589513\n",
      "Episode 39/200, Total Reward: 58.07847150409782\n",
      "Episode 40/200, Total Reward: 64.76172678119144\n",
      "Episode 41/200, Total Reward: 56.5377927442707\n",
      "Episode 42/200, Total Reward: 64.75123564571169\n",
      "Episode 43/200, Total Reward: 61.89135660031778\n",
      "Episode 44/200, Total Reward: 64.41368904296462\n",
      "Episode 45/200, Total Reward: 63.698106538964346\n",
      "Episode 46/200, Total Reward: 65.9930074575102\n",
      "Episode 47/200, Total Reward: 64.56218502037814\n",
      "Episode 48/200, Total Reward: 63.997102939115884\n",
      "Episode 49/200, Total Reward: 66.40636826858608\n",
      "Episode 50/200, Total Reward: 64.55255783379229\n",
      "Episode 51/200, Total Reward: 62.3071426500267\n",
      "Episode 52/200, Total Reward: 63.07704038028818\n",
      "Episode 53/200, Total Reward: 65.57396386137509\n",
      "Episode 54/200, Total Reward: 56.38044393339938\n",
      "Episode 55/200, Total Reward: 64.62777217457298\n",
      "Episode 56/200, Total Reward: 66.38946728426896\n",
      "Episode 57/200, Total Reward: 63.159457097372\n",
      "Episode 58/200, Total Reward: 67.70667487315124\n",
      "Episode 59/200, Total Reward: 62.88785398354937\n",
      "Episode 60/200, Total Reward: 63.72436043190424\n",
      "Episode 61/200, Total Reward: 62.53548631835603\n",
      "Episode 62/200, Total Reward: 65.53885775224578\n",
      "Episode 63/200, Total Reward: 65.30865307818803\n",
      "Episode 64/200, Total Reward: 66.497428362685\n",
      "Episode 65/200, Total Reward: 65.60729825112773\n",
      "Episode 66/200, Total Reward: 63.302249724035946\n",
      "Episode 67/200, Total Reward: 65.71827354757227\n",
      "Episode 68/200, Total Reward: 65.6430536640781\n",
      "Episode 69/200, Total Reward: 62.89804817303606\n",
      "Episode 70/200, Total Reward: 63.88982223270402\n",
      "Episode 71/200, Total Reward: 64.6570486629552\n",
      "Episode 72/200, Total Reward: 60.580230999962794\n",
      "Episode 73/200, Total Reward: 63.475322796845745\n",
      "Episode 74/200, Total Reward: 65.82726680996274\n",
      "Episode 75/200, Total Reward: 31.789407064021873\n",
      "Episode 76/200, Total Reward: 48.97985854193739\n",
      "Episode 77/200, Total Reward: 66.02680654351767\n",
      "Episode 78/200, Total Reward: 63.93068536715836\n",
      "Episode 79/200, Total Reward: 66.13273721744739\n",
      "Episode 80/200, Total Reward: 53.99788608698675\n",
      "Episode 81/200, Total Reward: 57.474866240980006\n",
      "Episode 82/200, Total Reward: 66.41868370694462\n",
      "Episode 83/200, Total Reward: 68.03366957287501\n",
      "Episode 84/200, Total Reward: 61.69264260792427\n",
      "Episode 85/200, Total Reward: 49.02278612812368\n",
      "Episode 86/200, Total Reward: 64.89642655442603\n",
      "Episode 87/200, Total Reward: 63.43751859661265\n",
      "Episode 88/200, Total Reward: 61.3838006362225\n",
      "Episode 89/200, Total Reward: 64.58437917981986\n",
      "Episode 90/200, Total Reward: 61.05340578540102\n",
      "Episode 91/200, Total Reward: 63.07659532349262\n",
      "Episode 92/200, Total Reward: 64.23797580223456\n",
      "Episode 93/200, Total Reward: 62.67704764132266\n",
      "Episode 94/200, Total Reward: 64.18589483211582\n",
      "Episode 95/200, Total Reward: 61.72756369872803\n",
      "Episode 96/200, Total Reward: 64.33734109073696\n",
      "Episode 97/200, Total Reward: 59.84317799738055\n",
      "Episode 98/200, Total Reward: 63.792422101761495\n",
      "Episode 99/200, Total Reward: 66.32900388527128\n",
      "Episode 100/200, Total Reward: 62.53619397203304\n",
      "Episode 101/200, Total Reward: 61.904682742155856\n",
      "Episode 102/200, Total Reward: 64.1570243422606\n",
      "Episode 103/200, Total Reward: 64.77865347250835\n",
      "Episode 104/200, Total Reward: 62.48285680759459\n",
      "Episode 105/200, Total Reward: 61.31226103552354\n",
      "Episode 106/200, Total Reward: 66.98385344023623\n",
      "Episode 107/200, Total Reward: 66.10740840100252\n",
      "Episode 108/200, Total Reward: 65.66458046157224\n",
      "Episode 109/200, Total Reward: 61.90362096727726\n",
      "Episode 110/200, Total Reward: 57.314793599438474\n",
      "Episode 111/200, Total Reward: 61.608871034294594\n",
      "Episode 112/200, Total Reward: 63.47815672083341\n",
      "Episode 113/200, Total Reward: 64.89120095895036\n",
      "Episode 114/200, Total Reward: 51.51673626828993\n",
      "Episode 115/200, Total Reward: 63.05298764784624\n",
      "Episode 116/200, Total Reward: 51.823263659048735\n",
      "Episode 117/200, Total Reward: 67.23829641803295\n",
      "Episode 118/200, Total Reward: 64.15227576330584\n",
      "Episode 119/200, Total Reward: 60.81072594136022\n",
      "Episode 120/200, Total Reward: 66.58987891559997\n",
      "Episode 121/200, Total Reward: 63.072044496933145\n",
      "Episode 122/200, Total Reward: 64.35885761223024\n",
      "Episode 123/200, Total Reward: 58.86504188716199\n",
      "Episode 124/200, Total Reward: 63.18990026201061\n",
      "Episode 125/200, Total Reward: 64.56462746686523\n",
      "Episode 126/200, Total Reward: 65.06829881771907\n",
      "Episode 127/200, Total Reward: 65.85932070195817\n",
      "Episode 128/200, Total Reward: 63.46988568685123\n",
      "Episode 129/200, Total Reward: 61.394563308887456\n",
      "Episode 130/200, Total Reward: 67.14642399019084\n",
      "Episode 131/200, Total Reward: 66.16336316547437\n",
      "Episode 132/200, Total Reward: 62.66571236396444\n",
      "Episode 133/200, Total Reward: 65.31422542140058\n",
      "Episode 134/200, Total Reward: 62.30794977795945\n",
      "Episode 135/200, Total Reward: 63.71501229522919\n",
      "Episode 136/200, Total Reward: 63.30242867955565\n",
      "Episode 137/200, Total Reward: 65.85128853049841\n",
      "Episode 138/200, Total Reward: 52.051602934822725\n",
      "Episode 139/200, Total Reward: 60.93388011001848\n",
      "Episode 140/200, Total Reward: 61.176317186185756\n",
      "Episode 141/200, Total Reward: 64.70193735328527\n",
      "Episode 142/200, Total Reward: 62.53639452390488\n",
      "Episode 143/200, Total Reward: 67.89588736861663\n",
      "Episode 144/200, Total Reward: 66.13501847318847\n",
      "Episode 145/200, Total Reward: 63.67436431221132\n",
      "Episode 146/200, Total Reward: 64.88777485276889\n",
      "Episode 147/200, Total Reward: 63.90385097988733\n",
      "Episode 148/200, Total Reward: 33.279437034598\n",
      "Episode 149/200, Total Reward: 64.62520820329011\n",
      "Episode 150/200, Total Reward: 64.80248172857809\n",
      "Episode 151/200, Total Reward: 66.31395765706598\n",
      "Episode 152/200, Total Reward: 63.276282545120175\n",
      "Episode 153/200, Total Reward: 62.086324725955535\n",
      "Episode 154/200, Total Reward: 64.4002273686967\n",
      "Episode 155/200, Total Reward: 63.69759161162389\n",
      "Episode 156/200, Total Reward: 63.95354787344295\n",
      "Episode 157/200, Total Reward: 66.0773076873318\n",
      "Episode 158/200, Total Reward: 64.76206393257003\n",
      "Episode 159/200, Total Reward: 66.39035129230619\n",
      "Episode 160/200, Total Reward: 63.77436630284045\n",
      "Episode 161/200, Total Reward: 68.12875517060492\n",
      "Episode 162/200, Total Reward: 55.8150156790843\n",
      "Episode 163/200, Total Reward: 62.96982975716446\n",
      "Episode 164/200, Total Reward: 62.40128390552251\n",
      "Episode 165/200, Total Reward: 65.16785849710347\n",
      "Episode 166/200, Total Reward: 66.073192209471\n",
      "Episode 167/200, Total Reward: 65.1803458946103\n",
      "Episode 168/200, Total Reward: 57.34622714813692\n",
      "Episode 169/200, Total Reward: 67.85550369290229\n",
      "Episode 170/200, Total Reward: 63.889552613820044\n",
      "Episode 171/200, Total Reward: 64.97251549075064\n",
      "Episode 172/200, Total Reward: 64.49552910459285\n",
      "Episode 173/200, Total Reward: 67.10767678020525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 174/200, Total Reward: 60.64999750023026\n",
      "Episode 175/200, Total Reward: 65.80010444020374\n",
      "Episode 176/200, Total Reward: 62.20840860280586\n",
      "Episode 177/200, Total Reward: 65.29530739522134\n",
      "Episode 178/200, Total Reward: 64.14367946640381\n",
      "Episode 179/200, Total Reward: 63.90848543395281\n",
      "Episode 180/200, Total Reward: 40.99223213462931\n",
      "Episode 181/200, Total Reward: 65.67917259577735\n",
      "Episode 182/200, Total Reward: 55.93034079535421\n",
      "Episode 183/200, Total Reward: 58.65107993835439\n",
      "Episode 184/200, Total Reward: 64.23156197677184\n",
      "Episode 185/200, Total Reward: 65.11439144340387\n",
      "Episode 186/200, Total Reward: 63.15886051526089\n",
      "Episode 187/200, Total Reward: 65.02620366493652\n",
      "Episode 188/200, Total Reward: 61.28596184099354\n",
      "Episode 189/200, Total Reward: 62.30181914149778\n",
      "Episode 190/200, Total Reward: 64.82876443431279\n",
      "Episode 191/200, Total Reward: 63.80437474241287\n",
      "Episode 192/200, Total Reward: 65.91487482793562\n",
      "Episode 193/200, Total Reward: 65.27333772764743\n",
      "Episode 194/200, Total Reward: 40.242358057587275\n",
      "Episode 195/200, Total Reward: 57.789190670645596\n",
      "Episode 196/200, Total Reward: 63.39999454430527\n",
      "Episode 197/200, Total Reward: 62.60920943720936\n",
      "Episode 198/200, Total Reward: 63.60010554666101\n",
      "Episode 199/200, Total Reward: 62.648370332399416\n",
      "Episode 200/200, Total Reward: 64.39541897198649\n"
     ]
    }
   ],
   "source": [
    "state_history_fake_0, rewards_list_fake_0 = train_dqn(agent_fake_0, env_fake_0, ALL_EPISODES, batch_size ,UPDATE,dqn_agent_name = \"Fake_diffusion_0.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e186bb",
   "metadata": {},
   "source": [
    "### FAKE_DIFF_ITER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a0d2af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE_DIFF_ITER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f8d61c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_train_graph_fake_1,active_fake_set_fake_1,intial_fake_set_fake_1,all_fake_nodes_fake_1 = generate_random_graph(FAKE_DIFF_ITER,M_INDEX,FAKE_SEED_NUM,NODE_NUM, EDGE_NUM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5425c0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 335, 47, 8, 45, 392, 0]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(active_fake_set_fake_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "552a1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "SEED_SIZE = int((FAKE_DIFF_ITER/(FAKE_DIFF_ITER+1))*len(active_fake_set_fake_1))\n",
    "print(SEED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c29920bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fake_1 = Env(intial_train_graph_fake_1, SEED_SIZE,active_fake_set_fake_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dea9c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fake_1 = DQNAgent(state_dim, action_dim, LR , GAMMA, EPSILON, EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "48f5e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Total Reward: 18.673832925763932\n",
      "Episode 2/200, Total Reward: 29.833857985867112\n",
      "Episode 3/200, Total Reward: 29.94464260922894\n",
      "Episode 4/200, Total Reward: 25.685440167415177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/bz9f8bp52sv9260n6zps4vbr0000gn/T/ipykernel_1713/545150970.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5/200, Total Reward: 34.81669420594481\n",
      "Episode 6/200, Total Reward: 40.396485961361876\n",
      "Episode 7/200, Total Reward: 20.872214814640238\n",
      "Episode 8/200, Total Reward: 35.12481752503651\n",
      "Episode 9/200, Total Reward: 32.16006602658109\n",
      "Episode 10/200, Total Reward: 35.315701317808006\n",
      "Episode 11/200, Total Reward: 34.445798570317365\n",
      "Episode 12/200, Total Reward: 40.5452885077405\n",
      "Episode 13/200, Total Reward: 15.3596632689966\n",
      "Episode 14/200, Total Reward: 30.331622153922012\n",
      "Episode 15/200, Total Reward: 35.83687826326873\n",
      "Episode 16/200, Total Reward: 26.90916220131822\n",
      "Episode 17/200, Total Reward: 33.5359717317661\n",
      "Episode 18/200, Total Reward: 30.757820258071025\n",
      "Episode 19/200, Total Reward: 28.34118085729048\n",
      "Episode 20/200, Total Reward: 27.278212454116186\n",
      "Episode 21/200, Total Reward: 26.762321976672332\n",
      "Episode 22/200, Total Reward: 20.513501601609242\n",
      "Episode 23/200, Total Reward: 33.34626126067744\n",
      "Episode 24/200, Total Reward: 20.685277392334804\n",
      "Episode 25/200, Total Reward: 39.089871042689666\n",
      "Episode 26/200, Total Reward: 30.365690328593768\n",
      "Episode 27/200, Total Reward: 21.85126232384234\n",
      "Episode 28/200, Total Reward: 32.064852878180666\n",
      "Episode 29/200, Total Reward: 22.560988364571873\n",
      "Episode 30/200, Total Reward: 29.7200596124212\n",
      "Episode 31/200, Total Reward: 29.25833988139951\n",
      "Episode 32/200, Total Reward: 26.994823723361005\n",
      "Episode 33/200, Total Reward: 28.986682156815505\n",
      "Episode 34/200, Total Reward: 16.072190653199634\n",
      "Episode 35/200, Total Reward: 40.346203573723265\n",
      "Episode 36/200, Total Reward: 33.146208460752035\n",
      "Episode 37/200, Total Reward: 29.469490370775837\n",
      "Episode 38/200, Total Reward: 19.45631646938123\n",
      "Episode 39/200, Total Reward: 29.03606139730716\n",
      "Episode 40/200, Total Reward: 5.508739337726684\n",
      "Episode 41/200, Total Reward: 31.169861245412566\n",
      "Episode 42/200, Total Reward: 37.17790730972542\n",
      "Episode 43/200, Total Reward: 40.847414438113375\n",
      "Episode 44/200, Total Reward: 30.46658191821738\n",
      "Episode 45/200, Total Reward: 28.681317988185903\n",
      "Episode 46/200, Total Reward: 26.23846257321849\n",
      "Episode 47/200, Total Reward: 28.62469827942636\n",
      "Episode 48/200, Total Reward: 17.869327584740077\n",
      "Episode 49/200, Total Reward: 24.35686907028206\n",
      "Episode 50/200, Total Reward: 35.21303727903879\n",
      "Episode 51/200, Total Reward: 29.505230090342597\n",
      "Episode 52/200, Total Reward: 32.521131066297734\n",
      "Episode 53/200, Total Reward: 26.584441719708938\n",
      "Episode 54/200, Total Reward: 15.483599424946306\n",
      "Episode 55/200, Total Reward: 22.71246472496016\n",
      "Episode 56/200, Total Reward: 28.975072763776275\n",
      "Episode 57/200, Total Reward: 32.41975756487114\n",
      "Episode 58/200, Total Reward: 33.77393086248049\n",
      "Episode 59/200, Total Reward: 30.686932920072465\n",
      "Episode 60/200, Total Reward: 30.431278078559124\n",
      "Episode 61/200, Total Reward: 24.24058136760309\n",
      "Episode 62/200, Total Reward: 33.498300561454\n",
      "Episode 63/200, Total Reward: 17.94423339676808\n",
      "Episode 64/200, Total Reward: 27.086943239793666\n",
      "Episode 65/200, Total Reward: 26.896305993862832\n",
      "Episode 66/200, Total Reward: 34.34292899932681\n",
      "Episode 67/200, Total Reward: 33.75720684719627\n",
      "Episode 68/200, Total Reward: 31.191430177089515\n",
      "Episode 69/200, Total Reward: 18.232004504984808\n",
      "Episode 70/200, Total Reward: 30.18463300811927\n",
      "Episode 71/200, Total Reward: 36.865165182388886\n",
      "Episode 72/200, Total Reward: 24.338893328527206\n",
      "Episode 73/200, Total Reward: 30.277221102244084\n",
      "Episode 74/200, Total Reward: 34.96944100417031\n",
      "Episode 75/200, Total Reward: 34.859843287436696\n",
      "Episode 76/200, Total Reward: 34.527824617546834\n",
      "Episode 77/200, Total Reward: 32.441766686436075\n",
      "Episode 78/200, Total Reward: 31.5299636292784\n",
      "Episode 79/200, Total Reward: 32.3492359657555\n",
      "Episode 80/200, Total Reward: 27.900638133925845\n",
      "Episode 81/200, Total Reward: 35.97973883230215\n",
      "Episode 82/200, Total Reward: 11.667263654081175\n",
      "Episode 83/200, Total Reward: 31.807763213936113\n",
      "Episode 84/200, Total Reward: 28.475383727192575\n",
      "Episode 85/200, Total Reward: 16.122622126736815\n",
      "Episode 86/200, Total Reward: 29.673477496251017\n",
      "Episode 87/200, Total Reward: 23.096226338901463\n",
      "Episode 88/200, Total Reward: 26.140869847585982\n",
      "Episode 89/200, Total Reward: 19.919110200419723\n",
      "Episode 90/200, Total Reward: 36.235007822272316\n",
      "Episode 91/200, Total Reward: 14.042512393643541\n",
      "Episode 92/200, Total Reward: 29.819831500831125\n",
      "Episode 93/200, Total Reward: 25.319189091641114\n",
      "Episode 94/200, Total Reward: 32.77039098808696\n",
      "Episode 95/200, Total Reward: 31.746217039128094\n",
      "Episode 96/200, Total Reward: 28.60087524114119\n",
      "Episode 97/200, Total Reward: 36.50405310313884\n",
      "Episode 98/200, Total Reward: 34.150369759936964\n",
      "Episode 99/200, Total Reward: 32.526847425411624\n",
      "Episode 100/200, Total Reward: 17.686626903869062\n",
      "Episode 101/200, Total Reward: 25.98434252722906\n",
      "Episode 102/200, Total Reward: 26.575988345406405\n",
      "Episode 103/200, Total Reward: 30.245754452909562\n",
      "Episode 104/200, Total Reward: 27.547430596069887\n",
      "Episode 105/200, Total Reward: 38.69562698659124\n",
      "Episode 106/200, Total Reward: 25.103564841200672\n",
      "Episode 107/200, Total Reward: 31.054256739157438\n",
      "Episode 108/200, Total Reward: 28.519608049971417\n",
      "Episode 109/200, Total Reward: 29.31601367180458\n",
      "Episode 110/200, Total Reward: 37.975543243118196\n",
      "Episode 111/200, Total Reward: 35.288807927195315\n",
      "Episode 112/200, Total Reward: 33.82274996114556\n",
      "Episode 113/200, Total Reward: 31.836523471360167\n",
      "Episode 114/200, Total Reward: 34.95968112629304\n",
      "Episode 115/200, Total Reward: 30.948482104907086\n",
      "Episode 116/200, Total Reward: 24.184614993804498\n",
      "Episode 117/200, Total Reward: 27.20421275542614\n",
      "Episode 118/200, Total Reward: 12.26355466937396\n",
      "Episode 119/200, Total Reward: 30.40664472956596\n",
      "Episode 120/200, Total Reward: 37.31199722784673\n",
      "Episode 121/200, Total Reward: 25.42747185959291\n",
      "Episode 122/200, Total Reward: 44.750823827505194\n",
      "Episode 123/200, Total Reward: 35.41894206881368\n",
      "Episode 124/200, Total Reward: 30.715068330939065\n",
      "Episode 125/200, Total Reward: 26.800803229212956\n",
      "Episode 126/200, Total Reward: 33.92078043091858\n",
      "Episode 127/200, Total Reward: 28.567233361240753\n",
      "Episode 128/200, Total Reward: 26.560113711085513\n",
      "Episode 129/200, Total Reward: 20.934776462439817\n",
      "Episode 130/200, Total Reward: 30.531174537856483\n",
      "Episode 131/200, Total Reward: 24.103204977396313\n",
      "Episode 132/200, Total Reward: 24.831280329622516\n",
      "Episode 133/200, Total Reward: 29.00184111174534\n",
      "Episode 134/200, Total Reward: 30.102015735694494\n",
      "Episode 135/200, Total Reward: 35.52101944681629\n",
      "Episode 136/200, Total Reward: 30.968581756127584\n",
      "Episode 137/200, Total Reward: 31.393750465806544\n",
      "Episode 138/200, Total Reward: 26.294261517401925\n",
      "Episode 139/200, Total Reward: 22.70491880925863\n",
      "Episode 140/200, Total Reward: 25.158347139881712\n",
      "Episode 141/200, Total Reward: 17.03375989233576\n",
      "Episode 142/200, Total Reward: 30.19706982486342\n",
      "Episode 143/200, Total Reward: 32.51238874042296\n",
      "Episode 144/200, Total Reward: 32.235900204735195\n",
      "Episode 145/200, Total Reward: 30.94700105875772\n",
      "Episode 146/200, Total Reward: 22.91674872391959\n",
      "Episode 147/200, Total Reward: 29.127381135050243\n",
      "Episode 148/200, Total Reward: 38.998108725864206\n",
      "Episode 149/200, Total Reward: 32.09204973958062\n",
      "Episode 150/200, Total Reward: 32.60769614065228\n",
      "Episode 151/200, Total Reward: 28.892781055675844\n",
      "Episode 152/200, Total Reward: 26.412032708637014\n",
      "Episode 153/200, Total Reward: 29.6629383355802\n",
      "Episode 154/200, Total Reward: 34.0821389582857\n",
      "Episode 155/200, Total Reward: 15.605094054745456\n",
      "Episode 156/200, Total Reward: 34.16801374094543\n",
      "Episode 157/200, Total Reward: 34.4915498936073\n",
      "Episode 158/200, Total Reward: 25.441726907390862\n",
      "Episode 159/200, Total Reward: 34.01288583693582\n",
      "Episode 160/200, Total Reward: 27.837615697499594\n",
      "Episode 161/200, Total Reward: 28.65534612214335\n",
      "Episode 162/200, Total Reward: 31.622748372892566\n",
      "Episode 163/200, Total Reward: 34.167817625205636\n",
      "Episode 164/200, Total Reward: 38.1621057147399\n",
      "Episode 165/200, Total Reward: 39.8931493892089\n",
      "Episode 166/200, Total Reward: 29.217398107544398\n",
      "Episode 167/200, Total Reward: 36.51959600356106\n",
      "Episode 168/200, Total Reward: 29.283156324029882\n",
      "Episode 169/200, Total Reward: 32.68907773205697\n",
      "Episode 170/200, Total Reward: 28.614284044430818\n",
      "Episode 171/200, Total Reward: 30.444712707885063\n",
      "Episode 172/200, Total Reward: 28.707297306310245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 173/200, Total Reward: 31.642790423872725\n",
      "Episode 174/200, Total Reward: 31.221005898820188\n",
      "Episode 175/200, Total Reward: 36.833334905987975\n",
      "Episode 176/200, Total Reward: 29.346807060370764\n",
      "Episode 177/200, Total Reward: 32.57314072859579\n",
      "Episode 178/200, Total Reward: 32.971000555175564\n",
      "Episode 179/200, Total Reward: 26.828035353535572\n",
      "Episode 180/200, Total Reward: 35.96389602501776\n",
      "Episode 181/200, Total Reward: 30.87917434000289\n",
      "Episode 182/200, Total Reward: 31.746918376483233\n",
      "Episode 183/200, Total Reward: 30.320024580862295\n",
      "Episode 184/200, Total Reward: 31.75809979602751\n",
      "Episode 185/200, Total Reward: 24.149920970380528\n",
      "Episode 186/200, Total Reward: 26.812451768474588\n",
      "Episode 187/200, Total Reward: 22.596109760368755\n",
      "Episode 188/200, Total Reward: 37.287717620071085\n",
      "Episode 189/200, Total Reward: 22.931460385478218\n",
      "Episode 190/200, Total Reward: 24.215089384759267\n",
      "Episode 191/200, Total Reward: 34.778305799354015\n",
      "Episode 192/200, Total Reward: 25.580876811224854\n",
      "Episode 193/200, Total Reward: 35.121273396441886\n",
      "Episode 194/200, Total Reward: 33.47918183055995\n",
      "Episode 195/200, Total Reward: 23.721350908140497\n",
      "Episode 196/200, Total Reward: 20.162877957947863\n",
      "Episode 197/200, Total Reward: 32.024864067169695\n",
      "Episode 198/200, Total Reward: 33.966502446703636\n",
      "Episode 199/200, Total Reward: 33.72709105681845\n",
      "Episode 200/200, Total Reward: 33.29447602327646\n"
     ]
    }
   ],
   "source": [
    "state_history_fake_1, rewards_list_fake_1 = train_dqn(agent_fake_1, env_fake_1, ALL_EPISODES, batch_size ,UPDATE,dqn_agent_name = \"Fake_diffusion_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64abe458",
   "metadata": {},
   "source": [
    "### FAKE_DIFF_ITER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7d01a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE_DIFF_ITER = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "697bbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_train_graph_fake_2,active_fake_set_fake_2,intial_fake_set_fake_2,all_fake_nodes_fake_2 = generate_random_graph(FAKE_DIFF_ITER,M_INDEX,FAKE_SEED_NUM,NODE_NUM, EDGE_NUM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "ba55278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(active_fake_set_fake_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "59a16d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "SEED_SIZE = int((FAKE_DIFF_ITER/(FAKE_DIFF_ITER+1))*len(active_fake_set_fake_2))\n",
    "print(SEED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0550d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fake_2 = Env(intial_train_graph_fake_2, SEED_SIZE,active_fake_set_fake_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "86111e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fake_2 = DQNAgent(state_dim, action_dim, LR , GAMMA, EPSILON, EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "08a66dca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Total Reward: 38.82978741278198\n",
      "Episode 2/200, Total Reward: 35.34110335348612\n",
      "Episode 3/200, Total Reward: 41.07210500078771\n",
      "Episode 4/200, Total Reward: 35.03957278830744\n",
      "Episode 5/200, Total Reward: 31.641004837120384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/bz9f8bp52sv9260n6zps4vbr0000gn/T/ipykernel_1713/545150970.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6/200, Total Reward: 36.52358108894868\n",
      "Episode 7/200, Total Reward: 22.83778053900418\n",
      "Episode 8/200, Total Reward: 37.05777897303151\n",
      "Episode 9/200, Total Reward: 38.10903290721129\n",
      "Episode 10/200, Total Reward: 32.851707439985475\n",
      "Episode 11/200, Total Reward: 34.579547943191855\n",
      "Episode 12/200, Total Reward: 34.071563355097965\n",
      "Episode 13/200, Total Reward: 32.87465760878479\n",
      "Episode 14/200, Total Reward: 29.52930859202153\n",
      "Episode 15/200, Total Reward: 35.05821633482655\n",
      "Episode 16/200, Total Reward: 35.0470331149927\n",
      "Episode 17/200, Total Reward: 35.29833071895637\n",
      "Episode 18/200, Total Reward: 32.23605115928899\n",
      "Episode 19/200, Total Reward: 19.05888464889813\n",
      "Episode 20/200, Total Reward: 33.19788114483864\n",
      "Episode 21/200, Total Reward: 33.77970544257377\n",
      "Episode 22/200, Total Reward: 33.282345359206566\n",
      "Episode 23/200, Total Reward: 37.68352236773057\n",
      "Episode 24/200, Total Reward: 32.76351215174568\n",
      "Episode 25/200, Total Reward: 34.22351962494009\n",
      "Episode 26/200, Total Reward: 23.491032721150255\n",
      "Episode 27/200, Total Reward: 43.32669209303523\n",
      "Episode 28/200, Total Reward: 31.47337864774628\n",
      "Episode 29/200, Total Reward: 35.20395988782523\n",
      "Episode 30/200, Total Reward: 35.096864100101605\n",
      "Episode 31/200, Total Reward: 35.74520456684405\n",
      "Episode 32/200, Total Reward: 37.5983806457215\n",
      "Episode 33/200, Total Reward: 31.286783799119895\n",
      "Episode 34/200, Total Reward: 37.83461883031124\n",
      "Episode 35/200, Total Reward: 34.76121571184331\n",
      "Episode 36/200, Total Reward: 22.376009160647563\n",
      "Episode 37/200, Total Reward: 29.700571100134464\n",
      "Episode 38/200, Total Reward: 32.95772966678399\n",
      "Episode 39/200, Total Reward: 33.86352668615838\n",
      "Episode 40/200, Total Reward: 35.89304984192597\n",
      "Episode 41/200, Total Reward: 35.51048709734906\n",
      "Episode 42/200, Total Reward: 37.02103985990157\n",
      "Episode 43/200, Total Reward: 32.764177567109954\n",
      "Episode 44/200, Total Reward: 33.44557108150554\n",
      "Episode 45/200, Total Reward: 34.96835003632479\n",
      "Episode 46/200, Total Reward: 34.914016041405155\n",
      "Episode 47/200, Total Reward: 39.2251475771673\n",
      "Episode 48/200, Total Reward: 31.688069020986713\n",
      "Episode 49/200, Total Reward: 33.51337492344846\n",
      "Episode 50/200, Total Reward: 32.85618742277178\n",
      "Episode 51/200, Total Reward: 33.04318269920041\n",
      "Episode 52/200, Total Reward: 27.91881870258272\n",
      "Episode 53/200, Total Reward: 37.56917055021489\n",
      "Episode 54/200, Total Reward: 34.51352784419813\n",
      "Episode 55/200, Total Reward: 33.43418675924997\n",
      "Episode 56/200, Total Reward: 34.77040390409815\n",
      "Episode 57/200, Total Reward: 30.57648665280901\n",
      "Episode 58/200, Total Reward: 32.798766603120406\n",
      "Episode 59/200, Total Reward: 32.38416937188906\n",
      "Episode 60/200, Total Reward: 34.290485153376416\n",
      "Episode 61/200, Total Reward: 25.339622931477102\n",
      "Episode 62/200, Total Reward: 44.07057019311968\n",
      "Episode 63/200, Total Reward: 36.390036234271676\n",
      "Episode 64/200, Total Reward: 36.555182850969246\n",
      "Episode 65/200, Total Reward: 39.450092414491145\n",
      "Episode 66/200, Total Reward: 30.62668341471069\n",
      "Episode 67/200, Total Reward: 35.76858524540052\n",
      "Episode 68/200, Total Reward: 31.72915956552822\n",
      "Episode 69/200, Total Reward: 32.330676795051374\n",
      "Episode 70/200, Total Reward: 34.239306058575586\n",
      "Episode 71/200, Total Reward: 36.41073911158088\n",
      "Episode 72/200, Total Reward: 35.5959092925455\n",
      "Episode 73/200, Total Reward: 34.457767332203574\n",
      "Episode 74/200, Total Reward: 40.889955592663725\n",
      "Episode 75/200, Total Reward: 33.788039230540576\n",
      "Episode 76/200, Total Reward: 33.4321994390713\n",
      "Episode 77/200, Total Reward: 30.943299847083203\n",
      "Episode 78/200, Total Reward: 35.97344986007715\n",
      "Episode 79/200, Total Reward: 37.476253546079114\n",
      "Episode 80/200, Total Reward: 34.444178068271796\n",
      "Episode 81/200, Total Reward: 32.28063275423501\n",
      "Episode 82/200, Total Reward: 38.640312428160854\n",
      "Episode 83/200, Total Reward: 41.01977498357935\n",
      "Episode 84/200, Total Reward: 33.2690641748798\n",
      "Episode 85/200, Total Reward: 35.34648276069519\n",
      "Episode 86/200, Total Reward: 36.996074612138536\n",
      "Episode 87/200, Total Reward: 34.42056696764635\n",
      "Episode 88/200, Total Reward: 30.002092722915954\n",
      "Episode 89/200, Total Reward: 32.65214587622642\n",
      "Episode 90/200, Total Reward: 31.597232849134326\n",
      "Episode 91/200, Total Reward: 37.70117878184423\n",
      "Episode 92/200, Total Reward: 38.78524645012348\n",
      "Episode 93/200, Total Reward: 40.36613541876281\n",
      "Episode 94/200, Total Reward: 36.11911627680816\n",
      "Episode 95/200, Total Reward: 35.54922784674094\n",
      "Episode 96/200, Total Reward: 35.581069825976805\n",
      "Episode 97/200, Total Reward: 29.581028613618397\n",
      "Episode 98/200, Total Reward: 32.5624969441147\n",
      "Episode 99/200, Total Reward: 42.648923822920324\n",
      "Episode 100/200, Total Reward: 35.17419666831175\n",
      "Episode 101/200, Total Reward: 25.106115236980678\n",
      "Episode 102/200, Total Reward: 40.75520270787599\n",
      "Episode 103/200, Total Reward: 29.887490697373796\n",
      "Episode 104/200, Total Reward: 36.03541396392153\n",
      "Episode 105/200, Total Reward: 34.943926677505985\n",
      "Episode 106/200, Total Reward: 35.73267108216295\n",
      "Episode 107/200, Total Reward: 37.27222134729334\n",
      "Episode 108/200, Total Reward: 35.509656798206514\n",
      "Episode 109/200, Total Reward: 36.2817163469369\n",
      "Episode 110/200, Total Reward: 32.74386275759211\n",
      "Episode 111/200, Total Reward: 35.200286134677974\n",
      "Episode 112/200, Total Reward: 32.296838953675945\n",
      "Episode 113/200, Total Reward: 39.981013566629414\n",
      "Episode 114/200, Total Reward: 36.13454589754953\n",
      "Episode 115/200, Total Reward: 38.14739484982185\n",
      "Episode 116/200, Total Reward: 36.421372872015446\n",
      "Episode 117/200, Total Reward: 35.00277646241751\n",
      "Episode 118/200, Total Reward: 35.33129883045301\n",
      "Episode 119/200, Total Reward: 34.966259788483164\n",
      "Episode 120/200, Total Reward: 32.29418477406629\n",
      "Episode 121/200, Total Reward: 36.19243566898098\n",
      "Episode 122/200, Total Reward: 38.023073654022056\n",
      "Episode 123/200, Total Reward: 26.997656672787006\n",
      "Episode 124/200, Total Reward: 33.70131129627953\n",
      "Episode 125/200, Total Reward: 36.56805425561286\n",
      "Episode 126/200, Total Reward: 34.809708876420224\n",
      "Episode 127/200, Total Reward: 35.08319886672148\n",
      "Episode 128/200, Total Reward: 33.71123865534356\n",
      "Episode 129/200, Total Reward: 39.05831458255299\n",
      "Episode 130/200, Total Reward: 34.4684192119373\n",
      "Episode 131/200, Total Reward: 39.014397944397814\n",
      "Episode 132/200, Total Reward: 31.486438937602003\n",
      "Episode 133/200, Total Reward: 34.464710270194544\n",
      "Episode 134/200, Total Reward: 38.23036701566477\n",
      "Episode 135/200, Total Reward: 34.62038434368001\n",
      "Episode 136/200, Total Reward: 33.95570014632861\n",
      "Episode 137/200, Total Reward: 37.84955556251816\n",
      "Episode 138/200, Total Reward: 36.796203709904965\n",
      "Episode 139/200, Total Reward: 31.136929211141112\n",
      "Episode 140/200, Total Reward: 41.04014214222762\n",
      "Episode 141/200, Total Reward: 34.32648753380583\n",
      "Episode 142/200, Total Reward: 33.41079732428166\n",
      "Episode 143/200, Total Reward: 35.822625259133666\n",
      "Episode 144/200, Total Reward: 42.239318275619965\n",
      "Episode 145/200, Total Reward: 38.60768517547204\n",
      "Episode 146/200, Total Reward: 33.50573834142136\n",
      "Episode 147/200, Total Reward: 28.970459720962346\n",
      "Episode 148/200, Total Reward: 27.596103900111377\n",
      "Episode 149/200, Total Reward: 35.696442516422586\n",
      "Episode 150/200, Total Reward: 38.607430182109795\n",
      "Episode 151/200, Total Reward: 37.60566609066276\n",
      "Episode 152/200, Total Reward: 39.53048824037441\n",
      "Episode 153/200, Total Reward: 36.361498550453895\n",
      "Episode 154/200, Total Reward: 36.44577098004688\n",
      "Episode 155/200, Total Reward: 34.31022065289042\n",
      "Episode 156/200, Total Reward: 25.649572914980617\n",
      "Episode 157/200, Total Reward: 32.39853868088801\n",
      "Episode 158/200, Total Reward: 34.11070837670868\n",
      "Episode 159/200, Total Reward: 38.68874514087682\n",
      "Episode 160/200, Total Reward: 33.34869748342058\n",
      "Episode 161/200, Total Reward: 35.907706894943125\n",
      "Episode 162/200, Total Reward: 37.63470173762448\n",
      "Episode 163/200, Total Reward: 34.190476998316655\n",
      "Episode 164/200, Total Reward: 33.4566096169029\n",
      "Episode 165/200, Total Reward: 39.718096049346585\n",
      "Episode 166/200, Total Reward: 36.70453970060344\n",
      "Episode 167/200, Total Reward: 41.919616855993254\n",
      "Episode 168/200, Total Reward: 31.61555247086399\n",
      "Episode 169/200, Total Reward: 22.072528052805318\n",
      "Episode 170/200, Total Reward: 35.2174485850835\n",
      "Episode 171/200, Total Reward: 38.69127207527851\n",
      "Episode 172/200, Total Reward: 33.84952401019267\n",
      "Episode 173/200, Total Reward: 40.43615898664707\n",
      "Episode 174/200, Total Reward: 30.292911391705825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 175/200, Total Reward: 35.31990206756514\n",
      "Episode 176/200, Total Reward: 34.93795067196057\n",
      "Episode 177/200, Total Reward: 34.02799452939482\n",
      "Episode 178/200, Total Reward: 32.371295371191415\n",
      "Episode 179/200, Total Reward: 32.13452136356969\n",
      "Episode 180/200, Total Reward: 28.677014848786335\n",
      "Episode 181/200, Total Reward: 40.98778680422389\n",
      "Episode 182/200, Total Reward: 41.088903858198535\n",
      "Episode 183/200, Total Reward: 37.92553662405281\n",
      "Episode 184/200, Total Reward: 32.73902671998211\n",
      "Episode 185/200, Total Reward: 30.181016943117722\n",
      "Episode 186/200, Total Reward: 41.110244285925816\n",
      "Episode 187/200, Total Reward: 36.42784266047765\n",
      "Episode 188/200, Total Reward: 33.13801557736983\n",
      "Episode 189/200, Total Reward: 21.382888825732792\n",
      "Episode 190/200, Total Reward: 31.906770321347643\n",
      "Episode 191/200, Total Reward: 33.605968233921104\n",
      "Episode 192/200, Total Reward: 35.301176907010245\n",
      "Episode 193/200, Total Reward: 31.764025198879594\n",
      "Episode 194/200, Total Reward: 29.416159335609734\n",
      "Episode 195/200, Total Reward: 26.661741244989656\n",
      "Episode 196/200, Total Reward: 31.77757028918208\n",
      "Episode 197/200, Total Reward: 34.592930162711156\n",
      "Episode 198/200, Total Reward: 35.99420932161279\n",
      "Episode 199/200, Total Reward: 33.660358626755716\n",
      "Episode 200/200, Total Reward: 44.08681625094339\n"
     ]
    }
   ],
   "source": [
    "state_history_fake_2, rewards_list_fake_2 = train_dqn(agent_fake_2, env_fake_2, ALL_EPISODES, batch_size ,UPDATE,dqn_agent_name = \"Fake_diffusion_2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83338330",
   "metadata": {},
   "source": [
    "### FAKE_DIFF_ITER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0007125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE_DIFF_ITER = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "036a8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_train_graph_fake_3,active_fake_set_fake_3,intial_fake_set_fake_3,all_fake_nodes_fake_3 = generate_random_graph(FAKE_DIFF_ITER,M_INDEX,FAKE_SEED_NUM,NODE_NUM, EDGE_NUM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "43ccc937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "SEED_SIZE = int((FAKE_DIFF_ITER/(FAKE_DIFF_ITER+1))*len(active_fake_set_fake_3))\n",
    "print(SEED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5b8cdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_fake_3 = Env(intial_train_graph_fake_3, SEED_SIZE,active_fake_set_fake_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f0124331",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fake_3 = DQNAgent(state_dim, action_dim, LR , GAMMA, EPSILON, EPSILON_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141b2817",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m state_history_fake_3, rewards_list_fake_3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m(agent_fake_3, env_fake_3, ALL_EPISODES, batch_size ,UPDATE,dqn_agent_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFake_diffusion_3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dqn' is not defined"
     ]
    }
   ],
   "source": [
    "state_history_fake_3, rewards_list_fake_3 = train_dqn(agent_fake_3, env_fake_3, ALL_EPISODES, batch_size ,UPDATE,dqn_agent_name = \"Fake_diffusion_3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ceb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3981f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
